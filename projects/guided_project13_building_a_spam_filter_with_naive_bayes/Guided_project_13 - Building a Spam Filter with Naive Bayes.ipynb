{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Filter with Naive Bayes\n",
    "\n",
    "In this project, we're going to study the practical side of the algorithm by building a spam filter for SMS messages.\n",
    "\n",
    "To classify messages as spam or non-spam, we saw that the computer:\n",
    "\n",
    "- Learns how humans classify messages.\n",
    "- Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "- Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "So our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the The [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). You can also download the dataset directly [from this link](https://dq-content.s3.amazonaws.com/433/SMSSpamCollection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers.\n",
    "\n",
    "Let's start by reading in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5571, 2)\n",
      "\n",
      "\n",
      "    ham  \\\n",
      "0   ham   \n",
      "1  spam   \n",
      "2   ham   \n",
      "3   ham   \n",
      "4  spam   \n",
      "\n",
      "  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
      "0                      Ok lar... Joking wif u oni...                                                               \n",
      "1  Free entry in 2 a wkly comp to win FA Cup fina...                                                               \n",
      "2  U dun say so early hor... U c already then say...                                                               \n",
      "3  Nah I don't think he goes to usf, he lives aro...                                                               \n",
      "4  FreeMsg Hey there darling it's been 3 week's n...                                                               \n",
      "\n",
      "\n",
      "       ham  \\\n",
      "5566  spam   \n",
      "5567   ham   \n",
      "5568   ham   \n",
      "5569   ham   \n",
      "5570   ham   \n",
      "\n",
      "     Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
      "5566  This is the 2nd time we have tried 2 contact u...                                                               \n",
      "5567               Will ü b going to esplanade fr home?                                                               \n",
      "5568  Pity, * was in mood for that. So...any other s...                                                               \n",
      "5569  The guy did some bitching but I acted like i'd...                                                               \n",
      "5570                         Rofl. Its true to its name                                                               \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "spam_collection = pd.read_csv('SMSSpamCollection', sep='\\t')\n",
    "print(spam_collection.shape)\n",
    "print('\\n')\n",
    "print(spam_collection.head())\n",
    "print('\\n')\n",
    "print(spam_collection.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file structure is very simple. The first column contains information about how a human rater classified an SMS message, either as 'spam' or as 'ham' (not spam). The second columb contains the text message as strings. We will assign column names to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message\n",
      "0   ham                      Ok lar... Joking wif u oni...\n",
      "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "2   ham  U dun say so early hor... U c already then say...\n",
      "3   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "4  spam  FreeMsg Hey there darling it's been 3 week's n...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5571</td>\n",
       "      <td>5571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4824</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5571                    5571\n",
       "unique     2                    5168\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4824                      30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_collection.columns = ['label', 'message']\n",
    "print(spam_collection.head())\n",
    "spam_collection.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     0.865913\n",
      "spam    0.134087\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(spam_collection['label'].value_counts(normalize='True'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 5571 labelled messages, of which 87% percent are being labelled as 'ham', so genuine non-spam messages. Amon the message content, we see that the most frequent unique message is \"Sorry, I'll call later\" (probably a preset response message)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test set\n",
    "\n",
    "In order to test how well our spam-filter works, we start by setting up a training set and a test set:\n",
    "- A training set, which we'll use to \"train\" the computer how to classify messages.\n",
    "- A test set, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,571 messages, which means that:\n",
    "\n",
    "- The training set will have 4,457 messages (about 80% of the dataset).\n",
    "- The test set will have 1,114 messages (about 20% of the dataset).\n",
    "\n",
    "For this project, our goal is to create a spam filter that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     0.865829\n",
      "spam    0.134171\n",
      "Name: label, dtype: float64\n",
      "\n",
      "\n",
      "ham     0.866248\n",
      "spam    0.133752\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Randomize the dataset\n",
    "randomized = spam_collection.sample(frac=1, random_state=1)\n",
    "randomized.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split into training set and test set\n",
    "training_set = randomized.iloc[0:4457, :]\n",
    "training_set.reset_index(drop=True, inplace=True)\n",
    "test_set = randomized.iloc[4457:, :]\n",
    "test_set.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(training_set['label'].value_counts(normalize='True'))\n",
    "print('\\n')\n",
    "print(test_set['label'].value_counts(normalize='True'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our randomization into the training and test set was successful. Each have the same share of spam as the original set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Letter case and punctuation\n",
    "\n",
    "The next big step is to use the training set to teach the algorithm to classify new messages. But we'll first need to perform a bit of data cleaning to bring the data in a format that will allow us to extract easily all the information we need. Let's begin the data cleaning process by removing the punctuation and bringing all the words to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Removing all punctuation from the message column\n",
    "training_set = training_set.copy()\n",
    "training_set.loc[:, 'no_punc'] = training_set['message'].apply(lambda x: re.sub(r'\\W', ' ', str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            message  \\\n",
      "0   ham           Dunno he jus say go lido. Same time 930.   \n",
      "1   ham  Slaaaaave ! Where are you ? Must I summon you ...   \n",
      "2  spam  Call from 08702490080 - tells u 2 call 0906635...   \n",
      "3  spam  You are guaranteed the latest Nokia Phone, a 4...   \n",
      "4   ham                          TaKe CaRE n gET WeLL sOOn   \n",
      "\n",
      "                                             no_punc  \n",
      "0           dunno he jus say go lido  same time 930   \n",
      "1  slaaaaave   where are you   must i summon you ...  \n",
      "2  call from 08702490080   tells u 2 call 0906635...  \n",
      "3  you are guaranteed the latest nokia phone  a 4...  \n",
      "4                          take care n get well soon  \n"
     ]
    }
   ],
   "source": [
    "# Setting everything into lower case\n",
    "training_set['no_punc'] = training_set['no_punc'].str.lower()\n",
    "\n",
    "print(training_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the original message column\n",
    "training_set.drop('message', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the vocabulary\n",
    "\n",
    "Eventually, we want to create a new column for every word in the messages. In order to achieve that, we need to create a set that contains the complete vocabulary in the messages. This is what we will do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing leading, trailing and multiple spaces\n",
    "training_set['cleaned_no_punc'] = training_set['no_punc'].apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "# Splitting every message up\n",
    "training_set['msg_content'] = training_set['cleaned_no_punc'].str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adding each word to a vocabulary list (training set only)\n",
    "vocabulary = []\n",
    "for item in training_set['msg_content']:\n",
    "    for word in item:\n",
    "        vocabulary.append(word)\n",
    "        \n",
    "# Transforming it to a set (removes duplicates)\n",
    "vocabulary = set(vocabulary)\n",
    "\n",
    "# Transforming back to list\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The final training set\n",
    "\n",
    "Having created the vocabulary, we now need to create a dictionary that counts each word in a message and appends the count as a value. This dictionary can then be turned into the formatted dataframe that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initalizing a dictionary\n",
    "# Each unique word gets a key and a list of 0 elements the length of the words in a messsage\n",
    "word_counts_per_sms = {unique_word: [0] * len(training_set['msg_content']) for unique_word in vocabulary}\n",
    "\n",
    "# Going through each word in the 'split' column, counting each word\n",
    "for index, sms in enumerate(training_set['msg_content']):\n",
    "    for word in sms:\n",
    "        word_counts_per_sms[word][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transforming this into a DataFrame\n",
    "word_counts_per_sms = pd.DataFrame(word_counts_per_sms)\n",
    "\n",
    "# Deleting empty column\n",
    "word_counts_per_sms = word_counts_per_sms.iloc[:, 1:]\n",
    "\n",
    "# Concatenating this with the training set\n",
    "training_set = pd.concat([training_set, word_counts_per_sms], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                            no_punc  \\\n",
      "0   ham           dunno he jus say go lido  same time 930    \n",
      "1   ham  slaaaaave   where are you   must i summon you ...   \n",
      "2  spam  call from 08702490080   tells u 2 call 0906635...   \n",
      "3  spam  you are guaranteed the latest nokia phone  a 4...   \n",
      "4   ham                          take care n get well soon   \n",
      "\n",
      "                                     cleaned_no_punc  \\\n",
      "0             dunno he jus say go lido same time 930   \n",
      "1  slaaaaave where are you must i summon you to m...   \n",
      "2  call from 08702490080 tells u 2 call 090663581...   \n",
      "3  you are guaranteed the latest nokia phone a 40...   \n",
      "4                          take care n get well soon   \n",
      "\n",
      "                                         msg_content  0  00  000  000pes  \\\n",
      "0   [dunno, he, jus, say, go, lido, same, time, 930]  0   0    0       0   \n",
      "1  [slaaaaave, where, are, you, must, i, summon, ...  0   0    0       0   \n",
      "2  [call, from, 08702490080, tells, u, 2, call, 0...  0   0    0       0   \n",
      "3  [you, are, guaranteed, the, latest, nokia, pho...  0   0    0       0   \n",
      "4                   [take, care, n, get, well, soon]  0   0    0       0   \n",
      "\n",
      "   008704050406  0089 ...  zed  zeros  zhong  zindgi  zoe  zoom  zyada  èn  é  \\\n",
      "0             0     0 ...    0      0      0       0    0     0      0   0  0   \n",
      "1             0     0 ...    0      0      0       0    0     0      0   0  0   \n",
      "2             0     0 ...    0      0      0       0    0     0      0   0  0   \n",
      "3             0     0 ...    0      0      0       0    0     0      0   0  0   \n",
      "4             0     0 ...    0      0      0       0    0     0      0   0  0   \n",
      "\n",
      "   ü  \n",
      "0  0  \n",
      "1  0  \n",
      "2  0  \n",
      "3  0  \n",
      "4  0  \n",
      "\n",
      "[5 rows x 7688 columns]\n"
     ]
    }
   ],
   "source": [
    "print(training_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating constants first\n",
    "\n",
    "Now that we're done with data cleaning and have a training set to work with, we can begin creating the spam filter. Recall that the Naive Bayes algorithm will need to know the probability values of the two equations below to be able to classify new messages. We will calculate all the necessary quantities now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8658290329818263\n"
     ]
    }
   ],
   "source": [
    "p_spam = training_set['label'].value_counts(normalize='True')[0]\n",
    "print(p_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13417096701817366\n"
     ]
    }
   ],
   "source": [
    "p_ham = training_set['label'].value_counts(normalize='True')[1]\n",
    "print(p_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_set['msg_n_words'] = training_set.iloc[:, 4:].gt(0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total words in spam messages\n",
    "n_spam = training_set[training_set['label'] == 'spam']['msg_n_words'].sum()\n",
    "\n",
    "# Total words in non-spam messages\n",
    "n_ham = training_set[training_set['label'] == 'ham']['msg_n_words'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total vocabulary\n",
    "n_vocabulary = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initiate a variable for Laplace smoothing\n",
    "alpha = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating parameters\n",
    "\n",
    "With these constant terms out of the way, we can calculate the probability parameters for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing two dictionaries to store the parameters\n",
    "del vocabulary[0] # There was an empty string in the dictionary\n",
    "parameters_spam = {unique_word: 0 for unique_word in vocabulary}\n",
    "parameters_ham  = {unique_word: 0 for unique_word in vocabulary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Isolate spam and ham meassages in seperate DataFrames\n",
    "spam = training_set[training_set['label'] == 'spam']\n",
    "spam = spam.iloc[:, 4:7688] # only retaining the word columns\n",
    "ham = training_set[training_set['label'] == 'ham']\n",
    "ham = ham.iloc[:, 4:7688] # only retaining the word columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate parameters\n",
    "for word in vocabulary:\n",
    "    n_word_given_spam = spam[word].sum()   \n",
    "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha*n_vocabulary)\n",
    "    parameters_spam[word] = p_word_given_spam\n",
    "    \n",
    "    n_word_given_ham = ham[word].sum()  \n",
    "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha*n_vocabulary)\n",
    "    parameters_ham[word] = p_word_given_ham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying a new message\n",
    "\n",
    "Now that we've calculated all the constants and parameters we need, we can start creating the spam filter. It will be a function that takes in a new message, calculates the relevant parameters, compares with the parameters that we already calculates and assess, whether it's more likely to be spam, ham, or if human help is needed.\n",
    "\n",
    "The function cleasn the incoming message data before calculating the parameters. If a message contains vocabulary that we don't have in our vocabulary list, it will be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def classify(message):\n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "    \n",
    "    message = re.sub('\\W', ' ', message) # remove punctuation\n",
    "    message = message.lower().split() # put into lower case\n",
    "    \n",
    "    p_spam_given_message = p_spam # initiate with some prior value\n",
    "    p_ham_given_message = p_ham # initiate with some prior value\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word] # update the prior with the parameter calculated before\n",
    "            \n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word] # update the prior with the parameter calculated before\n",
    "            \n",
    "    print('P(Spam|message):', p_spam_given_message)\n",
    "    print('P(Ham|message):', p_ham_given_message)\n",
    "    \n",
    "    if p_ham_given_message > p_spam_given_message: # compare p_spam_given_message with p_ham_given_message\n",
    "        print('Label: Ham')\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        print('Label: Spam')\n",
    "    else:\n",
    "        print('Equal proabilities, have a human classify this!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 6.5927014002685205e-25\n",
      "P(Ham|message): 1.084032851149266e-27\n",
      "Label: Spam\n"
     ]
    }
   ],
   "source": [
    "# Test with a spam message\n",
    "classify('WINNER!! This is the secret code to unlock the money: C3421.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 2.6554782486861542e-24\n",
      "P(Ham|message): 9.505319981703068e-22\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "# Test with a ham message\n",
    "classify('Sounds good, Tom, then see u there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Spam|message): 2.5753149323731747e-25\n",
      "P(Ham|message): 3.623916458242027e-21\n",
      "Label: Ham\n"
     ]
    }
   ],
   "source": [
    "# Test with a another ham message\n",
    "classify('Argh, I miss you! When are you coming?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the spam filter's accuracy\n",
    "\n",
    "We'll now try to determine how well the spam filter does on our test set of 1,114 messages.\n",
    "\n",
    "The algorithm will output a classification label for every message in our test set, which we'll be able to compare with the actual label (given by a human). Note that, in training, our algorithm didn't see these 1,114 messages, so every message in the test set is practically new from the perspective of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "\n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower()\n",
    "    message = message.split()\n",
    "\n",
    "    p_spam_given_message = p_spam\n",
    "    p_ham_given_message = p_ham\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "\n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_spam_given_message > p_ham_given_message:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'needs human classification'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function that returns labels instead of printing them, we can use it to create a new column in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dataquest/system/env/python3/lib/python3.4/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>S...from the training manual it show there is ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>LIFE has never been this much fun and great un...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Am new 2 club &amp; dont fink we met yet Will B gr...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Midnight at the earliest</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message predicted\n",
       "0   ham  S...from the training manual it show there is ...       ham\n",
       "1  spam  LIFE has never been this much fun and great un...       ham\n",
       "2  spam  Am new 2 club & dont fink we met yet Will B gr...      spam\n",
       "3   ham                           Midnight at the earliest       ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...       ham"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['predicted'] = test_set['message'].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1065\n",
      "Incorrect: 49\n",
      "Accuracy: 0.9560143626570916\n"
     ]
    }
   ],
   "source": [
    "# Measuring the accuracy\n",
    "correct = 0\n",
    "total = test_set.shape[0]\n",
    "\n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['label'] == row['predicted']:\n",
    "        correct += 1\n",
    "        \n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy:', correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is close to 95.60%, which is pretty good. Our spam filter looked at 1,114 messages that it hasn't seen in training, and classified 1,065 correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "In this project, we managed to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The filter had an accuracy of 95.60% on the test set we used, which is a pretty good result. Our initial goal was an accuracy of over 80%, and we managed to do way better than that.\n",
    "\n",
    "Next steps include:\n",
    "\n",
    "- Analyze the 49 messages that were classified incorrectly and try to figure out why the algorithm classified them incorrectly\n",
    "- Make the filtering process more complex by making the algorithm sensitive to letter case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
