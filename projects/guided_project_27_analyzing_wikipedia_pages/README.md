# Analyzing Wikipedia Pages

In this project, we'll work with data scraped from [Wikipedia](https://www.wikipedia.org/). Volunteer content contributors and editors maintain Wikipedia by continuously improving content. Anyone can edit Wikipedia (you can read more about how to make an edit [here](https://en.wikipedia.org/wiki/Help:Editing)). Because Wikipedia is crowdsourced, it has rapidly assembled a huge library of articles.

In this guided project, we'll implement a simplified version of the `grep` [command-line utility](https://en.wikipedia.org/wiki/Grep) to search for data in 54 megabytes worth of articles. If you're not familiar with the `grep` command, the `grep` utility essentially allows searching for textual data in all files from a given directory.

Articles were saved using the last component of their URLs. For example, a page on Wikipedia has the URL structure https://en.wikipedia.org/wiki/Yarkant_County. If we were saving the article with the previous URL, we'd save it to the file `Yarkant_County.html`. All the data files are in the wiki folder. Note that the files are raw HTML.

## Data Files
The project used a database (including datafiles) made available by [Dataquest.io](https://www.dataquest.io/). The code in the Jupyter notebook shows how to connect to that database. 

## Instructions

Download or clone this repository.
Ensure that the data files are located in the `Data/` directory.
Open the Jupyter Notebook `Guided Project 27 - Analyzing Wikipedia Pages.ipynb` and run the cells to reproduce the analysis.

## Project Notebook

You can also directly view or run the analysis in the [Jupyter Notebook](https://github.com/timmueller0/data_projects_misc/blob/main/projects/guided_project_27_analyzing_wikipedia_pages/Guided%20Project%2027%20-%20Analyzing%20Wikipedia%20Pages.ipynb)


