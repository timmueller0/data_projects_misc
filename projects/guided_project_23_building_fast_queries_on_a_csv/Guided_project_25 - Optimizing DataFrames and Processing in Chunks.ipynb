{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Optimizing DataFrames and Processing in Chunks\n",
    "\n",
    "In this project, we'll practice with chunked dataframes and optimize a dataframe's memory usage. We'll work with financial lending data from [Lending Club](https://www.lendingclub.com/), a marketplace for personal loans that matches borrowers with investors. You can read more about the marketplace [on its website](https://www.lendingclub.com/help/personal-loan-faq).\n",
    "\n",
    "The Lending Club's website lists approved loans. Qualified investors can view the borrower's credit score, the purpose of the loan, and other details in the loan applications. Once a lender is ready to back a loan, it selects the amount of money it wants to lend. When the loan amount the borrower requested is fully funded, the borrower receives the money, minus the origination fee that Lending Club charges.\n",
    "\n",
    "We'll work with a dataset of loans approved from 2007-2011. Although Lending Club no longer hosts the data, a comprehensive view of the data is available on Kaggle [here](https://www.kaggle.com/datasets/wordsforthewise/lending-club/data). The `desc` column has been removed to make the system run faster.\n",
    "\n",
    "If we read in the entire dataset, it consumes about 67 megabytes of memory. Let's imagine that we only have 10 megabytes of memory available throughout this project, so that everything needs to be processed in chunks. We'll start by reading in and checking the first five lines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>member_id</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>verification_status</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>pymnt_plan</th>\n",
       "      <th>purpose</th>\n",
       "      <th>title</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>addr_state</th>\n",
       "      <th>dti</th>\n",
       "      <th>delinq_2yrs</th>\n",
       "      <th>earliest_cr_line</th>\n",
       "      <th>inq_last_6mths</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_bal</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>initial_list_status</th>\n",
       "      <th>out_prncp</th>\n",
       "      <th>out_prncp_inv</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_pymnt_inv</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>total_rec_int</th>\n",
       "      <th>total_rec_late_fee</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>collection_recovery_fee</th>\n",
       "      <th>last_pymnt_d</th>\n",
       "      <th>last_pymnt_amnt</th>\n",
       "      <th>last_credit_pull_d</th>\n",
       "      <th>collections_12_mths_ex_med</th>\n",
       "      <th>policy_code</th>\n",
       "      <th>application_type</th>\n",
       "      <th>acc_now_delinq</th>\n",
       "      <th>chargeoff_within_12_mths</th>\n",
       "      <th>delinq_amnt</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1077501</td>\n",
       "      <td>1296599.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65%</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>Verified</td>\n",
       "      <td>Dec-2011</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>credit_card</td>\n",
       "      <td>Computer</td>\n",
       "      <td>860xx</td>\n",
       "      <td>AZ</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jan-1985</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13648.0</td>\n",
       "      <td>83.7%</td>\n",
       "      <td>9.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5863.155187</td>\n",
       "      <td>5833.84</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>863.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jan-2015</td>\n",
       "      <td>171.62</td>\n",
       "      <td>Jun-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1077430</td>\n",
       "      <td>1314167.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27%</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>Ryder</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2011</td>\n",
       "      <td>Charged Off</td>\n",
       "      <td>n</td>\n",
       "      <td>car</td>\n",
       "      <td>bike</td>\n",
       "      <td>309xx</td>\n",
       "      <td>GA</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Apr-1999</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>9.4%</td>\n",
       "      <td>4.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1008.710000</td>\n",
       "      <td>1008.71</td>\n",
       "      <td>456.46</td>\n",
       "      <td>435.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>117.08</td>\n",
       "      <td>1.11</td>\n",
       "      <td>Apr-2013</td>\n",
       "      <td>119.66</td>\n",
       "      <td>Sep-2013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077175</td>\n",
       "      <td>1313524.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>12252.0</td>\n",
       "      <td>Not Verified</td>\n",
       "      <td>Dec-2011</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>small_business</td>\n",
       "      <td>real estate business</td>\n",
       "      <td>606xx</td>\n",
       "      <td>IL</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Nov-2001</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2956.0</td>\n",
       "      <td>98.5%</td>\n",
       "      <td>10.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3005.666844</td>\n",
       "      <td>3005.67</td>\n",
       "      <td>2400.00</td>\n",
       "      <td>605.67</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jun-2014</td>\n",
       "      <td>649.91</td>\n",
       "      <td>Jun-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1076863</td>\n",
       "      <td>1277178.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49%</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>AIR RESOURCES BOARD</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>RENT</td>\n",
       "      <td>49200.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2011</td>\n",
       "      <td>Fully Paid</td>\n",
       "      <td>n</td>\n",
       "      <td>other</td>\n",
       "      <td>personel</td>\n",
       "      <td>917xx</td>\n",
       "      <td>CA</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feb-1996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5598.0</td>\n",
       "      <td>21%</td>\n",
       "      <td>37.0</td>\n",
       "      <td>f</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>12231.890000</td>\n",
       "      <td>12231.89</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>2214.92</td>\n",
       "      <td>16.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jan-2015</td>\n",
       "      <td>357.48</td>\n",
       "      <td>Apr-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075358</td>\n",
       "      <td>1311748.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>12.69%</td>\n",
       "      <td>67.79</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>University Medical Group</td>\n",
       "      <td>1 year</td>\n",
       "      <td>RENT</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Source Verified</td>\n",
       "      <td>Dec-2011</td>\n",
       "      <td>Current</td>\n",
       "      <td>n</td>\n",
       "      <td>other</td>\n",
       "      <td>Personal</td>\n",
       "      <td>972xx</td>\n",
       "      <td>OR</td>\n",
       "      <td>17.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Jan-1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27783.0</td>\n",
       "      <td>53.9%</td>\n",
       "      <td>38.0</td>\n",
       "      <td>f</td>\n",
       "      <td>461.73</td>\n",
       "      <td>461.73</td>\n",
       "      <td>3581.120000</td>\n",
       "      <td>3581.12</td>\n",
       "      <td>2538.27</td>\n",
       "      <td>1042.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Jun-2016</td>\n",
       "      <td>67.79</td>\n",
       "      <td>Jun-2016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>INDIVIDUAL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  member_id  loan_amnt  funded_amnt  funded_amnt_inv        term  \\\n",
       "0  1077501  1296599.0     5000.0       5000.0           4975.0   36 months   \n",
       "1  1077430  1314167.0     2500.0       2500.0           2500.0   60 months   \n",
       "2  1077175  1313524.0     2400.0       2400.0           2400.0   36 months   \n",
       "3  1076863  1277178.0    10000.0      10000.0          10000.0   36 months   \n",
       "4  1075358  1311748.0     3000.0       3000.0           3000.0   60 months   \n",
       "\n",
       "  int_rate  installment grade sub_grade                 emp_title emp_length  \\\n",
       "0   10.65%       162.87     B        B2                       NaN  10+ years   \n",
       "1   15.27%        59.83     C        C4                     Ryder   < 1 year   \n",
       "2   15.96%        84.33     C        C5                       NaN  10+ years   \n",
       "3   13.49%       339.31     C        C1       AIR RESOURCES BOARD  10+ years   \n",
       "4   12.69%        67.79     B        B5  University Medical Group     1 year   \n",
       "\n",
       "  home_ownership  annual_inc verification_status   issue_d  loan_status  \\\n",
       "0           RENT     24000.0            Verified  Dec-2011   Fully Paid   \n",
       "1           RENT     30000.0     Source Verified  Dec-2011  Charged Off   \n",
       "2           RENT     12252.0        Not Verified  Dec-2011   Fully Paid   \n",
       "3           RENT     49200.0     Source Verified  Dec-2011   Fully Paid   \n",
       "4           RENT     80000.0     Source Verified  Dec-2011      Current   \n",
       "\n",
       "  pymnt_plan         purpose                 title zip_code addr_state    dti  \\\n",
       "0          n     credit_card              Computer    860xx         AZ  27.65   \n",
       "1          n             car                  bike    309xx         GA   1.00   \n",
       "2          n  small_business  real estate business    606xx         IL   8.72   \n",
       "3          n           other              personel    917xx         CA  20.00   \n",
       "4          n           other              Personal    972xx         OR  17.94   \n",
       "\n",
       "   delinq_2yrs earliest_cr_line  inq_last_6mths  open_acc  pub_rec  revol_bal  \\\n",
       "0          0.0         Jan-1985             1.0       3.0      0.0    13648.0   \n",
       "1          0.0         Apr-1999             5.0       3.0      0.0     1687.0   \n",
       "2          0.0         Nov-2001             2.0       2.0      0.0     2956.0   \n",
       "3          0.0         Feb-1996             1.0      10.0      0.0     5598.0   \n",
       "4          0.0         Jan-1996             0.0      15.0      0.0    27783.0   \n",
       "\n",
       "  revol_util  total_acc initial_list_status  out_prncp  out_prncp_inv  \\\n",
       "0      83.7%        9.0                   f       0.00           0.00   \n",
       "1       9.4%        4.0                   f       0.00           0.00   \n",
       "2      98.5%       10.0                   f       0.00           0.00   \n",
       "3        21%       37.0                   f       0.00           0.00   \n",
       "4      53.9%       38.0                   f     461.73         461.73   \n",
       "\n",
       "    total_pymnt  total_pymnt_inv  total_rec_prncp  total_rec_int  \\\n",
       "0   5863.155187          5833.84          5000.00         863.16   \n",
       "1   1008.710000          1008.71           456.46         435.17   \n",
       "2   3005.666844          3005.67          2400.00         605.67   \n",
       "3  12231.890000         12231.89         10000.00        2214.92   \n",
       "4   3581.120000          3581.12          2538.27        1042.85   \n",
       "\n",
       "   total_rec_late_fee  recoveries  collection_recovery_fee last_pymnt_d  \\\n",
       "0                0.00        0.00                     0.00     Jan-2015   \n",
       "1                0.00      117.08                     1.11     Apr-2013   \n",
       "2                0.00        0.00                     0.00     Jun-2014   \n",
       "3               16.97        0.00                     0.00     Jan-2015   \n",
       "4                0.00        0.00                     0.00     Jun-2016   \n",
       "\n",
       "   last_pymnt_amnt last_credit_pull_d  collections_12_mths_ex_med  \\\n",
       "0           171.62           Jun-2016                         0.0   \n",
       "1           119.66           Sep-2013                         0.0   \n",
       "2           649.91           Jun-2016                         0.0   \n",
       "3           357.48           Apr-2016                         0.0   \n",
       "4            67.79           Jun-2016                         0.0   \n",
       "\n",
       "   policy_code application_type  acc_now_delinq  chargeoff_within_12_mths  \\\n",
       "0          1.0       INDIVIDUAL             0.0                       0.0   \n",
       "1          1.0       INDIVIDUAL             0.0                       0.0   \n",
       "2          1.0       INDIVIDUAL             0.0                       0.0   \n",
       "3          1.0       INDIVIDUAL             0.0                       0.0   \n",
       "4          1.0       INDIVIDUAL             0.0                       0.0   \n",
       "\n",
       "   delinq_amnt  pub_rec_bankruptcies  tax_liens  \n",
       "0          0.0                   0.0        0.0  \n",
       "1          0.0                   0.0        0.0  \n",
       "2          0.0                   0.0        0.0  \n",
       "3          0.0                   0.0        0.0  \n",
       "4          0.0                   0.0        0.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 99\n",
    "\n",
    "loans_chunk = pd.read_csv('loans_2007.csv', nrows=5)\n",
    "loans_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no apparent quality issues. We now read in 1000 rows and check the total memory usage. From that we can roughly estimate, how large the chunk size should be if we want to remain under 5 MB (to stay on the conservative side)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5273666381835938\n"
     ]
    }
   ],
   "source": [
    "loans_chunk = pd.read_csv('loans_2007.csv', nrows=1000)\n",
    "loans_chunk_memory_usage = loans_chunk.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "print(loans_chunk_memory_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 1000-row-chunk will take up approx. 1.57 MB, meaning that we can use chunks of 3000 rows each for further processing. Let's check how many rows we have in total for the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42538\n"
     ]
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "total_rows = 0\n",
    "for chunk in chunk_iter:\n",
    "    total_rows += len(chunk)\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's check how much memory is used overall (summing up over all chunks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.24251079559326\n"
     ]
    }
   ],
   "source": [
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "total_memory_usage = 0\n",
    "for chunk in chunk_iter:\n",
    "    total_memory_usage += chunk.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "print(total_memory_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data in Chunks\n",
    "\n",
    "Let's find out how many columns have a numeric and how many have a string type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 30, 30]\n",
      "[21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22]\n"
     ]
    }
   ],
   "source": [
    "# Counting numeric types by chunk\n",
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "numeric_counts =  [len(chunk.select_dtypes(['float64', 'int64']).columns) for chunk in chunk_iter]\n",
    "print(numeric_counts)\n",
    "\n",
    "# Counting string types by chunk\n",
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "string_counts =  [len(chunk.select_dtypes(['O']).columns) for chunk in chunk_iter]\n",
    "print(string_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that most of the time, 31 columns are counted as numeric, and 21 columns as string. Let's check which column(s) are responsible for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id']\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with inconsistent dtypes\n",
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "dtypes_list = [chunk.dtypes for chunk in chunk_iter]\n",
    "\n",
    "inconsistent_columns = set()\n",
    "for i in range(1, len(dtypes_list)):\n",
    "    diff = dtypes_list[i].compare(dtypes_list[i-1])\n",
    "    inconsistent_columns.update(diff.index)\n",
    "\n",
    "print(list(inconsistent_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `id`column, at least in the last two chunks seems to contain data that's being interpreted as string. This is not a crucial variable for analysis, and we will go on treating it as a string.\n",
    "\n",
    "Let's check how many non-missing values each string column has, how many of those are unique, and what the share is of unique values to total non-missing values in each string column. Those columns with less than 50% of unique values are candidates for a conversion into the categorical datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sub_grade': {'total_non_missing': 42535, 'unique_non_missing': 35, 'share_unique': 0.0008228517691313037}, 'home_ownership': {'total_non_missing': 42535, 'unique_non_missing': 5, 'share_unique': 0.00011755025273304338}, 'revol_util': {'total_non_missing': 42445, 'unique_non_missing': 1119, 'share_unique': 0.026363529273177054}, 'verification_status': {'total_non_missing': 42535, 'unique_non_missing': 3, 'share_unique': 7.053015163982603e-05}, 'emp_length': {'total_non_missing': 41423, 'unique_non_missing': 11, 'share_unique': 0.0002655529536730802}, 'term': {'total_non_missing': 42535, 'unique_non_missing': 2, 'share_unique': 4.702010109321735e-05}, 'pymnt_plan': {'total_non_missing': 42535, 'unique_non_missing': 2, 'share_unique': 4.702010109321735e-05}, 'earliest_cr_line': {'total_non_missing': 42506, 'unique_non_missing': 530, 'share_unique': 0.012468827930174564}, 'emp_title': {'total_non_missing': 39909, 'unique_non_missing': 30658, 'share_unique': 0.7681976496529604}, 'application_type': {'total_non_missing': 42535, 'unique_non_missing': 1, 'share_unique': 2.3510050546608674e-05}, 'issue_d': {'total_non_missing': 42535, 'unique_non_missing': 55, 'share_unique': 0.0012930527800634772}, 'id': {'total_non_missing': 42538, 'unique_non_missing': 42538, 'share_unique': 1.0}, 'purpose': {'total_non_missing': 42535, 'unique_non_missing': 14, 'share_unique': 0.00032914070765252144}, 'initial_list_status': {'total_non_missing': 42535, 'unique_non_missing': 1, 'share_unique': 2.3510050546608674e-05}, 'last_pymnt_d': {'total_non_missing': 42452, 'unique_non_missing': 103, 'share_unique': 0.0024262696692735324}, 'int_rate': {'total_non_missing': 42535, 'unique_non_missing': 394, 'share_unique': 0.009262959915363819}, 'zip_code': {'total_non_missing': 42535, 'unique_non_missing': 837, 'share_unique': 0.01967791230751146}, 'grade': {'total_non_missing': 42535, 'unique_non_missing': 7, 'share_unique': 0.00016457035382626072}, 'addr_state': {'total_non_missing': 42535, 'unique_non_missing': 50, 'share_unique': 0.0011755025273304338}, 'loan_status': {'total_non_missing': 42535, 'unique_non_missing': 9, 'share_unique': 0.0002115904549194781}, 'last_credit_pull_d': {'total_non_missing': 42531, 'unique_non_missing': 108, 'share_unique': 0.0025393242576003386}, 'title': {'total_non_missing': 42522, 'unique_non_missing': 21264, 'share_unique': 0.5000705517144066}}\n"
     ]
    }
   ],
   "source": [
    "# How many non-missing values in each string column?\n",
    "\n",
    "# Create a list of object columns\n",
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "obj_cols = set()  # any column that at least once is counted as object will be added to this set\n",
    "for chunk in chunk_iter:\n",
    "    obj_cols.update(chunk.select_dtypes(include=['object']).columns)\n",
    "obj_cols = list(obj_cols)  # convert set to list for easier handling\n",
    "\n",
    "# Count non-missing values, unique values and share of unique in each object column\n",
    "obj_dict = {}\n",
    "for col in obj_cols:\n",
    "    chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000, usecols=[col])\n",
    "    non_missing = [chunk[col].dropna() for chunk in chunk_iter]\n",
    "    combined_non_missing = pd.concat(non_missing)\n",
    "    total_non_missing = len(combined_non_missing)\n",
    "    unique_non_missing = combined_non_missing.nunique()\n",
    "    share_unique = unique_non_missing / total_non_missing\n",
    "    obj_dict[col] = {\n",
    "        'total_non_missing': total_non_missing,\n",
    "        'unique_non_missing': unique_non_missing,\n",
    "        'share_unique': share_unique\n",
    "    }\n",
    "\n",
    "print(obj_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this easier to read, let's print the columns with less than 50% unique and more than 50% unique values separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sub_grade': {'total_non_missing': 42535, 'unique_non_missing': 35, 'share_unique': 0.0008228517691313037}, 'home_ownership': {'total_non_missing': 42535, 'unique_non_missing': 5, 'share_unique': 0.00011755025273304338}, 'revol_util': {'total_non_missing': 42445, 'unique_non_missing': 1119, 'share_unique': 0.026363529273177054}, 'verification_status': {'total_non_missing': 42535, 'unique_non_missing': 3, 'share_unique': 7.053015163982603e-05}, 'emp_length': {'total_non_missing': 41423, 'unique_non_missing': 11, 'share_unique': 0.0002655529536730802}, 'term': {'total_non_missing': 42535, 'unique_non_missing': 2, 'share_unique': 4.702010109321735e-05}, 'pymnt_plan': {'total_non_missing': 42535, 'unique_non_missing': 2, 'share_unique': 4.702010109321735e-05}, 'earliest_cr_line': {'total_non_missing': 42506, 'unique_non_missing': 530, 'share_unique': 0.012468827930174564}, 'application_type': {'total_non_missing': 42535, 'unique_non_missing': 1, 'share_unique': 2.3510050546608674e-05}, 'issue_d': {'total_non_missing': 42535, 'unique_non_missing': 55, 'share_unique': 0.0012930527800634772}, 'purpose': {'total_non_missing': 42535, 'unique_non_missing': 14, 'share_unique': 0.00032914070765252144}, 'initial_list_status': {'total_non_missing': 42535, 'unique_non_missing': 1, 'share_unique': 2.3510050546608674e-05}, 'last_pymnt_d': {'total_non_missing': 42452, 'unique_non_missing': 103, 'share_unique': 0.0024262696692735324}, 'int_rate': {'total_non_missing': 42535, 'unique_non_missing': 394, 'share_unique': 0.009262959915363819}, 'zip_code': {'total_non_missing': 42535, 'unique_non_missing': 837, 'share_unique': 0.01967791230751146}, 'grade': {'total_non_missing': 42535, 'unique_non_missing': 7, 'share_unique': 0.00016457035382626072}, 'addr_state': {'total_non_missing': 42535, 'unique_non_missing': 50, 'share_unique': 0.0011755025273304338}, 'loan_status': {'total_non_missing': 42535, 'unique_non_missing': 9, 'share_unique': 0.0002115904549194781}, 'last_credit_pull_d': {'total_non_missing': 42531, 'unique_non_missing': 108, 'share_unique': 0.0025393242576003386}}\n",
      "\n",
      "\n",
      "{'emp_title': {'total_non_missing': 39909, 'unique_non_missing': 30658, 'share_unique': 0.7681976496529604}, 'id': {'total_non_missing': 42538, 'unique_non_missing': 42538, 'share_unique': 1.0}, 'title': {'total_non_missing': 42522, 'unique_non_missing': 21264, 'share_unique': 0.5000705517144066}}\n"
     ]
    }
   ],
   "source": [
    "# Only show the columns where the share of unique values is < 0.5\n",
    "filtered_dict = {k: v for k, v in obj_dict.items() if v['share_unique'] < 0.5}\n",
    "print(filtered_dict)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Only show the columns where the share of unique values is > 0.5\n",
    "filtered_dict = {k: v for k, v in obj_dict.items() if v['share_unique'] > 0.5}\n",
    "print(filtered_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the majority of string columns has much less than 50% unique values. Only `id`, `emp_title`, and `title` have so many unique values that assigning them the string-type makes sense. The other string-variables could potentially be converted to categorical variables.\n",
    "\n",
    "Let's check which of the float columns have no missing values and might be candidates for a conversion to the integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_rec_int': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'tax_liens': {'total_non_missing': np.int64(42430), 'share_non_missing': np.float64(0.9974845428685615)}, 'loan_amnt': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'out_prncp_inv': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'collection_recovery_fee': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'chargeoff_within_12_mths': {'total_non_missing': np.int64(42390), 'share_non_missing': np.float64(0.9965441850624163)}, 'dti': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'total_rec_late_fee': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'pub_rec_bankruptcies': {'total_non_missing': np.int64(41170), 'share_non_missing': np.float64(0.9678632719749864)}, 'installment': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'acc_now_delinq': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'collections_12_mths_ex_med': {'total_non_missing': np.int64(42390), 'share_non_missing': np.float64(0.9965441850624163)}, 'out_prncp': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'delinq_amnt': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'total_acc': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'delinq_2yrs': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'annual_inc': {'total_non_missing': np.int64(42531), 'share_non_missing': np.float64(0.9998589463290782)}, 'recoveries': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'total_pymnt_inv': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'member_id': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'total_pymnt': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'total_rec_prncp': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'funded_amnt_inv': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'pub_rec': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'revol_bal': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'inq_last_6mths': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'last_pymnt_amnt': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'funded_amnt': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'policy_code': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'open_acc': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}}\n"
     ]
    }
   ],
   "source": [
    "# Which float columns have no missing values?\n",
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "float_cols = set()\n",
    "for chunk in chunk_iter:\n",
    "    float_cols.update(chunk.select_dtypes(include=['float64']).columns)\n",
    "float_cols = list(float_cols)\n",
    "\n",
    "float_dict = {}\n",
    "for col in float_cols:\n",
    "    chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000, usecols=[col])\n",
    "    non_missing_cases = [chunk[~(chunk.isnull())].count() for chunk in chunk_iter]\n",
    "    total_non_missing = pd.concat(non_missing_cases).sum()\n",
    "    share_non_missing = total_non_missing / (total_rows - 1)\n",
    "    float_dict[col] = {\n",
    "        'total_non_missing': total_non_missing,\n",
    "        'share_non_missing': share_non_missing\n",
    "    }\n",
    "\n",
    "print(float_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total_rec_int': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'tax_liens': {'total_non_missing': np.int64(42430), 'share_non_missing': np.float64(0.9974845428685615)}, 'loan_amnt': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'out_prncp_inv': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'collection_recovery_fee': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'chargeoff_within_12_mths': {'total_non_missing': np.int64(42390), 'share_non_missing': np.float64(0.9965441850624163)}, 'dti': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'total_rec_late_fee': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'pub_rec_bankruptcies': {'total_non_missing': np.int64(41170), 'share_non_missing': np.float64(0.9678632719749864)}, 'installment': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'acc_now_delinq': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'collections_12_mths_ex_med': {'total_non_missing': np.int64(42390), 'share_non_missing': np.float64(0.9965441850624163)}, 'out_prncp': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'delinq_amnt': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'total_acc': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'delinq_2yrs': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'annual_inc': {'total_non_missing': np.int64(42531), 'share_non_missing': np.float64(0.9998589463290782)}, 'recoveries': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'total_pymnt_inv': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'member_id': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'total_pymnt': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'total_rec_prncp': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'funded_amnt_inv': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'pub_rec': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'revol_bal': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'inq_last_6mths': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}, 'last_pymnt_amnt': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'funded_amnt': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'policy_code': {'total_non_missing': np.int64(42535), 'share_non_missing': np.float64(0.9999529821096927)}, 'open_acc': {'total_non_missing': np.int64(42506), 'share_non_missing': np.float64(0.9992712227002375)}}\n"
     ]
    }
   ],
   "source": [
    "# Which float columns have no missing values?\n",
    "chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000)\n",
    "float_cols = set()\n",
    "for chunk in chunk_iter:\n",
    "    float_cols.update(chunk.select_dtypes(include=['float64']).columns)\n",
    "float_cols = list(float_cols)\n",
    "\n",
    "float_dict = {}\n",
    "for col in float_cols:\n",
    "    chunk_iter = pd.read_csv('loans_2007.csv', chunksize=3000, usecols=[col])\n",
    "    non_missing_cases = [chunk[~(chunk.isnull())].count() for chunk in chunk_iter]\n",
    "    total_non_missing = pd.concat(non_missing_cases).sum()\n",
    "    share_non_missing = total_non_missing / (total_rows - 1)\n",
    "    float_dict[col] = {\n",
    "        'total_non_missing': total_non_missing,\n",
    "        'share_non_missing': share_non_missing\n",
    "    }\n",
    "\n",
    "print(float_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, there are no floats with non-missing values. That's why we can't easily transform them to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Numeric Columns\n",
    "\n",
    "We can automatize the steps from above and write a function `load_optimized_dataframe` that will automatically assign optimal `dtypes` for each column. This is what we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_optimized_dataframe(filename, sample_fraction=0.1, chunksize=1000, use_float_for_nan_ints=False,\n",
    "                             use_float_for_nan_bools=False, encoding='latin1', **kwargs):\n",
    "    sample_list = []\n",
    "    chunk_iter = pd.read_csv(filename, encoding=encoding, chunksize=chunksize, **kwargs)\n",
    "\n",
    "    for chunk in chunk_iter:\n",
    "        sample_list.append(chunk.sample(frac=sample_fraction, random_state=1))\n",
    "\n",
    "    file_sample = pd.concat(sample_list).sample(frac=sample_fraction, random_state=1)\n",
    "\n",
    "    dtype_dict = {}\n",
    "    parse_dates = []\n",
    "\n",
    "    for col in file_sample.columns:\n",
    "        col_data = file_sample[col].dropna()\n",
    "\n",
    "        if col_data.dtype == 'object':\n",
    "            try:\n",
    "                pd.to_datetime(col_data)\n",
    "                parse_dates.append(col)\n",
    "            except (ValueError, TypeError):\n",
    "                if col_data.nunique() / len(col_data) < 0.5:\n",
    "                    dtype_dict[col] = 'category'\n",
    "\n",
    "        elif col_data.dtype == 'float64':\n",
    "            if col_data.mod(1).eq(0).all():\n",
    "                min_val, max_val = col_data.min(), col_data.max()\n",
    "\n",
    "                if set(col_data.unique()).issubset({0, 1}):\n",
    "                    dtype_dict[col] = 'float32' if use_float_for_nan_bools else 'boolean' if file_sample[\n",
    "                        col].isna().any() else 'bool'\n",
    "                else:\n",
    "                    for dtype in [np.int8, np.int16, np.int32, np.int64]:\n",
    "                        if np.iinfo(dtype).min <= min_val <= max_val <= np.iinfo(dtype).max:\n",
    "                            dtype_dict[col] = dtype\n",
    "                            break\n",
    "                    if file_sample[col].isna().any():\n",
    "                        dtype_dict[col] = 'float32' if use_float_for_nan_ints else 'Int64'\n",
    "            else:\n",
    "                dtype_dict[col] = 'float32' if col_data.abs().max() < np.finfo(np.float32).max else 'float64'\n",
    "\n",
    "        elif col_data.dtype == 'int64':\n",
    "            min_val, max_val = col_data.min(), col_data.max()\n",
    "\n",
    "            if set(col_data.unique()).issubset({0, 1}):\n",
    "                dtype_dict[col] = 'float32' if use_float_for_nan_bools else 'boolean' if file_sample[\n",
    "                    col].isna().any() else 'bool'\n",
    "            else:\n",
    "                for dtype in [np.int8, np.int16, np.int32, np.int64]:\n",
    "                    if np.iinfo(dtype).min <= min_val <= max_val <= np.iinfo(dtype).max:\n",
    "                        dtype_dict[col] = dtype\n",
    "                        break\n",
    "\n",
    "    df = pd.read_csv(filename, dtype=dtype_dict, parse_dates=parse_dates, encoding=encoding, **kwargs)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will load a chunk of data, use it as a sample and then determine the optimal `dtype`for each column. Since the date type is tested on all columns, this will lead to some user warnings, which we can ignore. Let's see how well this function will optimize our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:19: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  pd.to_datetime(col_data)\n",
      "/tmp/ipykernel_59/1381094617.py:54: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(filename, dtype=dtype_dict, parse_dates=parse_dates, encoding=encoding, **kwargs)\n",
      "/tmp/ipykernel_59/1381094617.py:54: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(filename, dtype=dtype_dict, parse_dates=parse_dates, encoding=encoding, **kwargs)\n",
      "/tmp/ipykernel_59/1381094617.py:54: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(filename, dtype=dtype_dict, parse_dates=parse_dates, encoding=encoding, **kwargs)\n",
      "/tmp/ipykernel_59/1381094617.py:54: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(filename, dtype=dtype_dict, parse_dates=parse_dates, encoding=encoding, **kwargs)\n",
      "/tmp/ipykernel_59/1381094617.py:54: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df = pd.read_csv(filename, dtype=dtype_dict, parse_dates=parse_dates, encoding=encoding, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42538 entries, 0 to 42537\n",
      "Data columns (total 52 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   id                          42538 non-null  object        \n",
      " 1   member_id                   42535 non-null  Int64         \n",
      " 2   loan_amnt                   42535 non-null  Int64         \n",
      " 3   funded_amnt                 42535 non-null  Int64         \n",
      " 4   funded_amnt_inv             42535 non-null  float32       \n",
      " 5   term                        42535 non-null  category      \n",
      " 6   int_rate                    42535 non-null  category      \n",
      " 7   installment                 42535 non-null  float32       \n",
      " 8   grade                       42535 non-null  category      \n",
      " 9   sub_grade                   42535 non-null  category      \n",
      " 10  emp_title                   39909 non-null  object        \n",
      " 11  emp_length                  41423 non-null  category      \n",
      " 12  home_ownership              42535 non-null  category      \n",
      " 13  annual_inc                  42531 non-null  float32       \n",
      " 14  verification_status         42535 non-null  category      \n",
      " 15  issue_d                     42535 non-null  datetime64[ns]\n",
      " 16  loan_status                 42535 non-null  category      \n",
      " 17  pymnt_plan                  42535 non-null  category      \n",
      " 18  purpose                     42535 non-null  category      \n",
      " 19  title                       42522 non-null  object        \n",
      " 20  zip_code                    42535 non-null  category      \n",
      " 21  addr_state                  42535 non-null  category      \n",
      " 22  dti                         42535 non-null  float32       \n",
      " 23  delinq_2yrs                 42506 non-null  Int64         \n",
      " 24  earliest_cr_line            42506 non-null  datetime64[ns]\n",
      " 25  inq_last_6mths              42506 non-null  Int64         \n",
      " 26  open_acc                    42506 non-null  Int64         \n",
      " 27  pub_rec                     42506 non-null  Int64         \n",
      " 28  revol_bal                   42535 non-null  Int64         \n",
      " 29  revol_util                  42445 non-null  category      \n",
      " 30  total_acc                   42506 non-null  Int64         \n",
      " 31  initial_list_status         42535 non-null  category      \n",
      " 32  out_prncp                   42535 non-null  float32       \n",
      " 33  out_prncp_inv               42535 non-null  float32       \n",
      " 34  total_pymnt                 42535 non-null  float32       \n",
      " 35  total_pymnt_inv             42535 non-null  float32       \n",
      " 36  total_rec_prncp             42535 non-null  float32       \n",
      " 37  total_rec_int               42535 non-null  float32       \n",
      " 38  total_rec_late_fee          42535 non-null  float32       \n",
      " 39  recoveries                  42535 non-null  float32       \n",
      " 40  collection_recovery_fee     42535 non-null  float32       \n",
      " 41  last_pymnt_d                42452 non-null  datetime64[ns]\n",
      " 42  last_pymnt_amnt             42535 non-null  float32       \n",
      " 43  last_credit_pull_d          42531 non-null  datetime64[ns]\n",
      " 44  collections_12_mths_ex_med  42390 non-null  boolean       \n",
      " 45  policy_code                 42535 non-null  boolean       \n",
      " 46  application_type            42535 non-null  category      \n",
      " 47  acc_now_delinq              42506 non-null  boolean       \n",
      " 48  chargeoff_within_12_mths    42390 non-null  boolean       \n",
      " 49  delinq_amnt                 42506 non-null  Int64         \n",
      " 50  pub_rec_bankruptcies        41170 non-null  Int64         \n",
      " 51  tax_liens                   42430 non-null  boolean       \n",
      "dtypes: Int64(11), boolean(5), category(15), datetime64[ns](4), float32(14), object(3)\n",
      "memory usage: 16.6 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "loans = load_optimized_dataframe('loans_2007.csv', sample_fraction=1, chunksize=3000)\n",
    "print(loans.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! From initially 62 MB of memory usage, we managed to bring this down to 16.6 MB, just by assigning the appropriate `dtypes`. Sweet!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
